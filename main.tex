\documentclass{article}
\usepackage{xcolor}

\title{Dynamically Scheduled High-Level Synthesis: a Synthesis}
\author{Lana Josipovi\'c, Radhika Ghosal and Paolo Ienne\\ Synthesis by Aur\`ele Barri\`ere}
\date{November 4th, 2018}

\def\todo#1{{\color{red}#1}}

\begin{document}
\maketitle

\section{Introduction}
As more and more companies and datacenters start using FPGAs, the programming paradigms should adapt to a broader range of applications.
The main goal of generating circuits for FPGAs is performance, and using parallelism as much as possible is a key to a shorter execution time.

High-level Synthesis (HLS) tools generate circuits for FPGAs, from languages such as C.
As of today, most HLS tools have a static approach to operation scheduling: the exact cycle where each operation will be executed is decided at synthesis time.
Despite rigorous research on these techniques, static scheduling is fundamentally unable to exploit parallelism in some cases.
For instance, when pipelining a loop, each iteration must follow the same schedule, even if the control-flow differs. To this end, static HLS must always assume the worst-case scenario: every branch might be taken and any two memory accesses might be dependent.

Such information (independence of memory access or whether or not a branch is executed) could be retrieved dynamically, at run-time. Using this information could allow more parallelism.
As a result, this paper presents a new strategy for HLS tools: generating dynamically svheduled circuits.
This would mean that the exact schedule is unknown at synthesis-time, allowing different iterations of a loop to be scheduled differently depending on run-time information.
Of course, extracting such information and delaying scheduling implies an overhead in time and an a bigger circuit complexity, but the overall performance should increase whenever static scheduling could not exploit some instruction parallelism.

The end result is a complete methodology to generate dynamically scheduled circuits from C code. The transformation is automatic and has been implemented, tested and compared with traditional static HLS tools and other dynamic scheduling techniques.


\todo{Parallel with VLIW and superscalar. Solution derives from VLIW tecniques? [28,37].
VLIW, and HLS is really successful for regular programs.}

\section{Dynamic Scheduling}
\todo{Give example where it's needed.}
Example where we can't decide statically whether some long computation will be done.
Static scheduling with loop pipelining has to have the same schedule for each loop.
No pipelining: we lose parralelism.
Solution: dynamic scheduling. We need to delay the choices. Adds an overhead.

Other example: use of address. We must assume worst-case.

\section{Elastic Circuits}

% Cortadella's paper
In [?], Cortadella et al define \textit{Elastic Circuits}, a new scheme for latency-insensitive designs.
Its primary motivation is to create a solution for circuits that might deal with variable delays, for modularity or scalability purposes for instance.
Indeed, if the delay between two connected functional units is not known, then we need to enforce synchronous behaviour: the receiver should wait until the data produced by the sender has been computed, and the sender should wait until the receiver is ready to accept it.
These new circuits implement the SELF protocol, where each unit is equipped with a \textit{Valid} and a \textit{Stop} Signal. When the sender's valid signal is true (meaning that its stored data is valid), and the receiver's stop signal is false (meaning that it can accept new data), then a data transfer occurs. Otherwise, the transfer is delayed. These signals are computed dynamically, and the protocol adds only a small overhead. The result is a circuit that enforces synchronicity, can be obtained automatically and proved to be correct by construction.

FIGURE?

Inspired by this definition, and [?], which brought elastic circuits to CGRAs, the authors suggest that using elastic circuits woul allow for a dynamic schedule in High-Level Synhesis.

\todo{List of components}
Such components include classical structure simply augmented with the appropriate handshake signals. For instance elastic buffers, elastic FIFOs, elastic Select (multiplexer).



\section{Synthesizing Elastic Circuits}

First step: basic blocks to datapath.\\
Implement control flow. Tokens.\\
Registers\\
Memory\\
FIFO

\section{Evaluation}
The authors implemented a basic synthesizer following the described techniques.
It uses clang for parsing and SSA generation, one of LLVM's intermediate representations and the LLVM standard optimizations.
The methodology itself is implemented as a transformation from LLVM's IR to an elastic circuit, itself transformed into a VHDL netlist. The end result can be run on a FPGA.

\subsection{Benchmark}
Most programs used in this benchmark are known to present parallelism issues when using static scheduling. This shows the best results one could expect when switching to dynamic scheduling.
The first program \textit{Histogram} goes through an array and increases histogram bars upon finding corresponding values. Statically, we can't determine whether accessing the bars may contain memory dependencies, so we need to assume the worst each time.
The benchmark also includes a \textit{Matrix Power} program, and two loops. The second loop changes the way an operation is performed to show how resource usage in Dynamic HLS can be reduced.
Finally, the benchmark include a \textit{FIR filter}, that is knwon to perform well under static synthesis. This programs allows to estimate the overhead introduced by dynamic scheduling, as no more parallelism can be achieved (every read and write is known to be independent at synthesis-time).

\subsection{Results}
These programs are first compared to Static HLS tools, such as Vivado HLS with pipelin optimization. Comparison fairness is enforced by using the exact same arithmetic units (extracted manually), the same RAMs.
Comparison is done through modelization (with ModelSim).
As no effort has been made yet to optimize the alstic circuits themselves, while static scheduling has benefited from years of study, one could expect to obtain even better results.

Results can be seen on Table ?. One can see significant improvements in execution time for most programs, despite an increase on clock period due to elastic handshake signals.
The results show that the dynamic overhead is negligible compared to the time spared with more parallelism. As expected, the only program where static scheduling performs better is the \textit{FIR filter}, but the difference is minor.
Of course, this execution time improvement requires an added circuit complexity. As a result, elastic circuits are less resource-friendly than static ones. The authors yet argue that the overhead of adding FIFOs would be much less prevalent in bigger programs. Some optimizations could still be done. For instance the second loop is designed to use much less units than the first one.

The new methodology is then compared to other dynamic scheduling techniques.
\todo{describe results}

\section{Conclusion}

\end{document}
